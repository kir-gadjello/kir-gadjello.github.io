<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing ZIPSLICERüìÅ‚úÇÔ∏è | Kirill Gadjello's personal blog and website</title><meta name=keywords content="zipslicer,intro,project,library,low-level,efficiency,checkpoint,storage,mlops"><meta name=description content="Finally! A library able to access PyTorch checkpoints without causing RAM outage [V1]"><meta name=author content="Kirill Gadjello"><link rel=canonical href=https://kir-gadjello.github.io/zipslicer><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://kir-gadjello.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kir-gadjello.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kir-gadjello.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kir-gadjello.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kir-gadjello.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Introducing ZIPSLICERüìÅ‚úÇÔ∏è"><meta property="og:description" content="Finally! A library able to access PyTorch checkpoints without causing RAM outage [V1]"><meta property="og:type" content="article"><meta property="og:url" content="https://kir-gadjello.github.io/posts/zipslicer/"><meta property="og:image" content="https://kir-gadjello.github.io/large_checkpoint_example.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-03T00:00:00+00:00"><meta property="og:site_name" content="Hopefully Generally Useful?"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kir-gadjello.github.io/large_checkpoint_example.png"><meta name=twitter:title content="Introducing ZIPSLICERüìÅ‚úÇÔ∏è"><meta name=twitter:description content="Finally! A library able to access PyTorch checkpoints without causing RAM outage [V1]"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kir-gadjello.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Introducing ZIPSLICERüìÅ‚úÇÔ∏è","item":"https://kir-gadjello.github.io/posts/zipslicer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing ZIPSLICERüìÅ‚úÇÔ∏è","name":"Introducing ZIPSLICERüìÅ‚úÇÔ∏è","description":"Finally! A library able to access PyTorch checkpoints without causing RAM outage [V1]","keywords":["zipslicer","intro","project","library","low-level","efficiency","checkpoint","storage","mlops"],"articleBody":"ZIPSLICER is available on GitHub and on PyPI. Intended audience: individuals who find themselves working with torch checkpoints with size on the order of available CPU RAM.\nIntroduction Successful software design and satisfactory performance have underpinned PyTorch‚Äôs rise to the top as the premier deep learning (hereafter DL) framework over the past 5 years. From this simple observation we can proceed to notice that the majority of DL models start the post-training part of their lifecycle as a monolithic pytorch checkpoint produced by a simple torch.save() call.\nThere is no shortage of infra problems with large ML models these days. Even if we take training out of the picture, one transient challenge which often arises in ML engineering is executing transformations on the trained checkpoints. Naturally, to do anything with the checkpoint at all, you need to be able to access ‚Äì load ‚Äì it. With the convenient and well-supported torch.load() mechanism, even very simple transformations like sharding the tensors and direct casting of weight datatypes consume some multiple (usually 4, due to single-precision floating point still being the only well-supported format on many CPUs) of the checkpointed model‚Äôs parameter count in RAM bytes. This 4x size multiplier is in stark contrast with the compressed in-VRAM size of these models after they are optimized for inference, which could be as low as 4 bits per parameter, with the usual being around 8 bits per parameter. When one works with large checkpoints, such as Galactica-120B, RAM consumption starts to become a major problem. In well-funded companies and institutions this problem is usually avoided through one of the few available options:\nRenting a high-RAM server in the cloud just for this specific part of the ML engineering pipeline; Distributing checkpoints as a list of shards (usually with an adhoc sharding config or scheme, and with each shard being quite large as well) ‚Äì so each shard ends up being a separate pytorch checkpoint; Or even by replacing the native torch storage format with something else, such as the novel safetensors format by huggingface. One could, of course, dismiss this as an issue concerning only the small circle of ML engineers whose professional tasks involve large language models. But I don‚Äôt think we should underestimate the proliferation of these large models into adjacent applied fields of science, entertainment and such; and while this could be downplayed as a mere tech demo, popularity of projects like FlexGen points to the popular demand for optimizing the foundations of this usecase.\nThe monolithic checkpoint problem in the age of LLMs Here are some obvious inadequacies in realistic scenarios of working with checkpoints today:\nOften enough you need to spin up multi-gigabyte checkpoints from huggingface and other sources ‚Äì and not just for inference, where you at least have access to a decently-sized cluster provided by your employer, but to execute some transient model transformations in preparation for that as well.\nModel conversion pipelines are often implemented in such a way as to require storing the complete checkpoint state at RAM at least once ‚Äì even though in principle this shouldn‚Äôt be necessary, as the model is already stored on the non-volatile storage.\nMany DL models, and specifically transformer neural networks, enjoy a very uniform layered architecture supporting large batch sizes: you can even run inference layer-by-layer, if your runtime engine supports it. And considering the industry demand for large-batch offline inference, there is a practical angle to it as well.\nThe naive implementation of the abovementioned runtime, while possible, suffers from the basic problem inherent to the current design of pytorch: the checkpoint writer uses large byte blobs for persisting the underlying tensor storage, and the default checkpoint loader has to load entire storage blobs even if you only need a few tensors.\nClearly, the fundamental bottleneck here lies in the torch.save() and, more importantly for most tasks, in torch.load() functions ‚Äì our hypothetical incremental model execution engine‚Äôs RAM savings would be made obsolete by the calls to stock torch.load().\nCan we write an alternative implementation of torch.load() (And torch.save(), eventually) as an External memory algorithm ?\nWhile it is true that pytorch checkpoint is a pickle ‚Äì a native Pythonic format for persisting stateful instances of objects ‚Äì packaged in a zipfile, and given the general-purpose nature of pickles this could greatly complicate the exercise we are getting to. Thankfully the typical torch checkpoint is a much simpler dictionary-like object called state_dict. Now we can assume a specific, but very common and officially recommended case of storing your model‚Äôs parameters as a state_dict ‚Äì which is simply a pickled Python OrderedDict with keys corresponding to name dependent on the position of the parent nn.Module in the hierarchy and the value is the tensor data (or plain extra state). This simplifies the problem, leading to a natural API for incremental tensor loading: we can provide a lightweight ordered dictionary of references to tensors stored in the checkpoint and materialize them by reading just enough of the zip file, when the users accesses the associated value. We can also provide tensor metadata by key without loading the whole tensor from disk.\nNaturally, this approach would require a customized pickle reader and tensor- and storage- loader. I took this task head-on, and in the next section I present the short rundown of problems I encountered on the way.\nThe fine structure of a Torch checkpoint We already noted that the checkpoint is a pickle ‚Äì but it is also packaged in a standard zipfile.\nSurely many people have tried to open a torch checkpoint with zip archiver; what they saw was a list of files with opaque names, one of them called ‚Äúdata.pkl‚Äù:\nMy first impulse, driven by curiosity, was to jump into torch serialization source straight from my editor, which is possible due to the opensource nature of PyTorch. I saw a nest of functions, some of which were used for an older folder-based data format. After some soul-searching, I decided that the way forward was to generously insert print() statements into important functions related to tensor assembly and unpickling; a couple of hours later I had a working dumper of function calls that are usually executed to construct tensors of the state_dict.\nUsing this dumper code and the base pytorch serialization source code as a template, I extracted the function tree of the torch.load() into a standalone source tree, which later became a part of zipslicer‚Äôs source code. Crucially, I patched the pickler to replace function calls which create massive storage objects and tensor objects with emitters of lightweight metadata.\nThis metadata is stored in the zipslicer object to be used as a direction for my relatively novel contribution ‚Äì a function that is able to correctly compute the required offset to get the portion of storage that is enough to represent the tensor, and actually does the work of navigating the binary zipfile stream in constant time to read it and return a tensor. The initial simple solution of using the default pytorch storage objects wasn‚Äôt cutting it, due to many tensors being coalesced into a single torch storage by torch.save(), at least in the checkpoints I was interested in - defeating the point of incremental loading. And while, in theory, there is a possibility of rare tensor types (such as pytorch native quantized tensors, or non-contiguous tensors) being incompatible with this simple, fast approach; and it also depends on the zipfile being uncompressed (which is the default in torch.save()) ‚Äì for now, my exhaustive tests for the checkpoints I have weren‚Äôt met with incompatibility.\nThis functionality ‚Äì a drop-in replacement for torch.load() for many compatible checkpoints ‚Äì is packaged in a high-level zipslicer.load() call, returning an OrderedDict-like LazyStateDict which implements the logic described above on the fly.\nThis is how the initial version of ZIPSLICER came to be. Hopefully in the next versions we will support overlay checkpoints (seamlessly multiplexing many checkpoints into one in a git-sourcetree-like fashion) and develop our own torch.save(). For now, if you want to save a large checkpoint you can simple save it as a list of smaller shards.\nI expect this release to make practical work with large deep learning models more approachable for students and under-resourced researchers alike.\nThe high-level API of ZIPSLICER: zipslicer.load() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import torch import zipslicer # Could be a private custom recurrent sentient transformer # instead of a garden variety resnet my_complicated_network = torch.hub.load( \"pytorch/vision:v0.10.0\", \"resnet18\", pretrained=True ) s_dict = my_complicated_network.state_dict() torch.save(s_dict, \"my_network_checkpoint_v123.pth\") del my_complicated_network # Later, on a smaller unrelated machine you load a \"LazyStateDict\" # Which is just like a regular state dict, but it loads tensors only # when it has to lazy_s_dict = zipslicer.load(\"my_network_checkpoint_v123.pth\") layer3_tensors = {} for k in lazy_s_dict.keys(): if k.startswith(\"layer3\"): layer3_tensors[k] = lazy_s_dict[k] # Now you have layer3's tensors, you can analyze them in your laptop's RAM. # Or you can instantiate the layers in sequence and compute the whole # network's output by threading the activations through them. # But we will just print the tensors instead: print(layer3_tensors) See this example in the GitHub repo\nCall to collaboration This is the first alpha-release of the library. Right now it works in a few scenarios of my personal interest, and there is a small-ish test suite and an exhaustive compatibility tester script. Any help at validating the loader for a wider range of usecases (with the provided read-only tester script) is welcome.\nThe author also expresses interest in supplying HF safetensors with an efficient conversion script based on ZIPSLICER.\nHonorable mentions zipcraft - for demonstrating easy addressing into an uncompressed zipfile HDF5 - for showing how it was done by our fathers Safetensors - for aspiration of simplicity, with a tint of rust indexed_zstd - for saving Earth, one bit at a time indexed_gzip - for compression of what we have Seeking-optimized ZIP - for aspiration to compress more ","wordCount":"1667","inLanguage":"en","image":"https://kir-gadjello.github.io/large_checkpoint_example.png","datePublished":"2023-03-03T00:00:00Z","dateModified":"2023-03-03T00:00:00Z","author":{"@type":"Person","name":"Kirill Gadjello"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kir-gadjello.github.io/posts/zipslicer/"},"publisher":{"@type":"Organization","name":"Kirill Gadjello's personal blog and website","logo":{"@type":"ImageObject","url":"https://kir-gadjello.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kir-gadjello.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://kir-gadjello.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kir-gadjello.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kir-gadjello.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://kir-gadjello.github.io/posts/>Posts</a></div><h1 class=post-title>Introducing ZIPSLICERüìÅ‚úÇÔ∏è</h1><div class=post-description>Finally! A library able to access PyTorch checkpoints without causing RAM outage [V1]</div><div class=post-meta><span title='2023-03-03 00:00:00 +0000 +0000'>March 3, 2023</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1667 words&nbsp;¬∑&nbsp;Kirill Gadjello&nbsp;|&nbsp;<a href=https://github.com/kir-gadjello/kir-gadjello/content/posts/zipslicer.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=lazy src=https://kir-gadjello.github.io/large_checkpoint_example.png alt="a screenshot of Alpa documentation showing 700GB CPU RAM requirements for checkpoint conversion"><p>Sometimes the LM checkpoint processing script&rsquo;s RAM requirements are entirely excessive</p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#the-monolithic-checkpoint-problem-in-the-age-of-llms>The monolithic checkpoint problem in the age of LLMs</a></li><li><a href=#the-fine-structure-of-a-torch-checkpoint>The fine structure of a Torch checkpoint</a></li><li><a href=#the-high-level-api-of-zipslicer-zipslicerload>The high-level API of ZIPSLICER: zipslicer.load()</a></li><li><a href=#call-to-collaboration>Call to collaboration</a></li><li><a href=#honorable-mentions>Honorable mentions</a></li></ul></nav></div></details></div><div class=post-content><p><a href=https://github.com/kir-gadjello/zipslicer>ZIPSLICER is available on GitHub</a> and on <a href=https://pypi.org/project/zipslicer/>PyPI</a>.
Intended audience: individuals who find themselves working with torch checkpoints with size on the order of available CPU RAM.</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Successful software design and satisfactory performance have underpinned <a href=https://pytorch.org/>PyTorch</a>&rsquo;s rise to the top as the premier deep learning (hereafter DL) framework over the past 5 years. From this simple observation we can proceed to notice that the majority of DL models start the post-training part of their lifecycle as a monolithic pytorch checkpoint produced by a simple <code>torch.save()</code> call.</p><p>There is no shortage of infra problems with large ML models these days. Even if we take training out of the picture, one transient challenge which often arises in ML engineering is executing transformations on the trained checkpoints. Naturally, to do anything with the checkpoint at all, you need to be able to access ‚Äì load ‚Äì it. With the convenient and well-supported <code>torch.load()</code> mechanism, even very simple transformations like sharding the tensors and direct casting of weight datatypes consume some multiple (usually 4, due to single-precision floating point still being the only well-supported format on many CPUs) of the checkpointed model&rsquo;s parameter count in RAM bytes. This 4x size multiplier is in stark contrast with the compressed in-VRAM size of these models after they are optimized for inference, <a href=https://arxiv.org/abs/2212.09720>which could be as low as 4 bits per parameter, with the usual being around 8 bits per parameter</a>. When one works with large checkpoints, such as <a href=https://huggingface.co/facebook/galactica-120b/tree/main>Galactica-120B</a>, RAM consumption starts to become a major problem. In well-funded companies and institutions this problem is usually avoided through one of the few available options:</p><ul><li>Renting a high-RAM server in the cloud just for this specific part of the ML engineering pipeline;</li><li>Distributing checkpoints as a list of shards (usually with an adhoc sharding config or scheme, and with each shard being quite large as well) ‚Äì so each shard ends up being a separate pytorch checkpoint;</li><li>Or even by replacing the native torch storage format with something else, such as the novel <code>safetensors</code> format by huggingface.</li></ul><p>One could, of course, dismiss this as an issue concerning only the small circle of ML engineers whose professional tasks involve large language models. But I don&rsquo;t think we should underestimate the proliferation of these large models into adjacent applied fields of science, entertainment and such; and while this could be downplayed as a mere tech demo, popularity of projects like <a href=https://github.com/FMInference/FlexGen>FlexGen</a> points to the popular demand for optimizing the foundations of this usecase.</p><h2 id=the-monolithic-checkpoint-problem-in-the-age-of-llms>The monolithic checkpoint problem in the age of LLMs<a hidden class=anchor aria-hidden=true href=#the-monolithic-checkpoint-problem-in-the-age-of-llms>#</a></h2><p>Here are some obvious inadequacies in realistic scenarios of working with checkpoints today:</p><ul><li><p>Often enough you need to spin up multi-gigabyte checkpoints from huggingface and other sources ‚Äì and not just for inference, where you at least have access to a decently-sized cluster provided by your employer, but to execute some transient model transformations in preparation for that as well.</p></li><li><p><a href=https://alpa.ai/tutorials/opt_serving.html#convert-opt-175b-weights-into-alpa-formats>Model conversion pipelines are often implemented in such a way as to require storing the complete checkpoint state at RAM at least once</a> ‚Äì even though <em>in principle</em> this shouldn&rsquo;t be necessary, as the model is already stored on the non-volatile storage.</p></li><li><p>Many DL models, and specifically transformer neural networks, enjoy a very uniform layered architecture supporting large batch sizes: you can even run inference layer-by-layer, <em>if your runtime engine supports it</em>. And considering the industry demand for large-batch offline inference, there is a practical angle to it as well.</p></li><li><p>The naive implementation of the abovementioned runtime, while possible, suffers from the basic problem inherent to the current design of pytorch: the checkpoint writer uses large byte blobs for persisting the underlying tensor storage, and the default checkpoint loader has to load entire storage blobs even if you only need a few tensors.</p></li></ul><p>Clearly, the fundamental bottleneck here lies in the <code>torch.save()</code> and, more importantly for most tasks, in <code>torch.load()</code> functions ‚Äì our hypothetical incremental model execution engine&rsquo;s RAM savings would be made obsolete by the calls to stock <code>torch.load()</code>.</p><p>Can we write an alternative implementation of <code>torch.load()</code> (And <code>torch.save()</code>, eventually) as an <a href=https://en.wikipedia.org/wiki/External_memory_algorithm>External memory algorithm</a> ?</p><p>While it is true that pytorch checkpoint is a <a href=https://docs.python.org/3/library/pickle.html>pickle ‚Äì a native Pythonic format for persisting stateful instances of objects</a> ‚Äì packaged in a zipfile, and given the general-purpose nature of pickles this could greatly complicate the exercise we are getting to. Thankfully the typical torch checkpoint is a much simpler dictionary-like object called <a href=https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html><code>state_dict</code></a>. Now we can assume a specific, but very common and officially recommended case of storing your model&rsquo;s parameters as a <code>state_dict</code> ‚Äì which is simply a pickled Python OrderedDict with keys corresponding to name dependent on the position of the parent nn.Module in the hierarchy and the value is the tensor data (or <a href=https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_extra_state>plain extra state</a>). This simplifies the problem, leading to a natural API for incremental tensor loading: we can provide a lightweight ordered dictionary of references to tensors stored in the checkpoint and materialize them by reading just enough of the zip file, when the users accesses the associated value. We can also provide tensor metadata by key without loading the whole tensor from disk.</p><p>Naturally, this approach would require a customized pickle reader and tensor- and storage- loader. I took this task head-on, and in the next section I present the short rundown of problems I encountered on the way.</p><h2 id=the-fine-structure-of-a-torch-checkpoint>The fine structure of a Torch checkpoint<a hidden class=anchor aria-hidden=true href=#the-fine-structure-of-a-torch-checkpoint>#</a></h2><p>We already noted that the checkpoint is a pickle ‚Äì but it is also packaged in a standard zipfile.</p><p>Surely many people have tried to open a <a href=https://huggingface.co/togethercomputer/GPT-JT-6B-v1/blob/main/pytorch_model.bin>torch checkpoint</a> with zip archiver; what they saw was a list of files with opaque names, one of them called &ldquo;data.pkl&rdquo;:</p><p><img loading=lazy src=/gpt_jt_v1_zip_overview.png alt="Inside GPT-JT-V1 checkpoint"></p><p>My first impulse, driven by curiosity, was to jump into <a href=https://github.com/pytorch/pytorch/blob/master/torch/serialization.py>torch serialization source</a> straight from my editor, which is possible due to the opensource nature of PyTorch. I saw a nest of functions, some of which were used for an older folder-based data format. After some soul-searching, I decided that the way forward was to generously insert <code>print()</code> statements into important functions related to tensor assembly and unpickling; a couple of hours later I had a working dumper of function calls that are usually executed to construct tensors of the <code>state_dict</code>.</p><p>Using this dumper code and the base pytorch serialization source code as a template, I extracted the function tree of the <code>torch.load()</code> into <a href=https://github.com/kir-gadjello/zipslicer/blob/main/zipslicer/custom_load.py>a standalone source tree</a>, which later became a part of zipslicer&rsquo;s source code. Crucially, I patched the pickler to replace function calls which create massive storage objects and tensor objects with emitters of lightweight metadata.</p><p>This metadata is stored in the zipslicer object to be used as a direction for <a href=https://github.com/kir-gadjello/zipslicer/blob/main/zipslicer/__init__.py#L124>my relatively novel contribution</a> ‚Äì a function that is able to correctly compute the required offset to get the portion of storage that is enough to represent the tensor, and actually does the work of navigating the binary zipfile stream in constant time to read it and return a tensor. The initial simple solution of using the default pytorch storage objects wasn&rsquo;t cutting it, due to many tensors being coalesced into a single torch storage by <code>torch.save()</code>, at least in the checkpoints I was interested in - defeating the point of incremental loading. And while, in theory, there is a possibility of rare tensor types (such as pytorch native quantized tensors, or non-contiguous tensors) being incompatible with this simple, fast approach; and it also depends on the zipfile being uncompressed (which is the default in <code>torch.save()</code>) ‚Äì for now, my exhaustive tests for the checkpoints I have weren&rsquo;t met with incompatibility.</p><p>This functionality ‚Äì a drop-in replacement for <code>torch.load()</code> for many compatible checkpoints ‚Äì is packaged in a high-level <code>zipslicer.load()</code> call, returning an OrderedDict-like <code>LazyStateDict</code> which implements the logic described above on the fly.</p><p>This is how the initial version of ZIPSLICER came to be. Hopefully in the next versions we will support overlay checkpoints (seamlessly multiplexing many checkpoints into one in a git-sourcetree-like fashion) and develop our own torch.save(). For now, if you want to save a large checkpoint you can simple save it as a list of smaller shards.</p><p>I expect this release to make practical work with large deep learning models more approachable for students and under-resourced researchers alike.</p><h2 id=the-high-level-api-of-zipslicer-zipslicerload>The high-level API of ZIPSLICER: zipslicer.load()<a hidden class=anchor aria-hidden=true href=#the-high-level-api-of-zipslicer-zipslicerload>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12>12</a>
</span><span class=lnt id=hl-0-13><a class=lnlinks href=#hl-0-13>13</a>
</span><span class=lnt id=hl-0-14><a class=lnlinks href=#hl-0-14>14</a>
</span><span class=lnt id=hl-0-15><a class=lnlinks href=#hl-0-15>15</a>
</span><span class=lnt id=hl-0-16><a class=lnlinks href=#hl-0-16>16</a>
</span><span class=lnt id=hl-0-17><a class=lnlinks href=#hl-0-17>17</a>
</span><span class=lnt id=hl-0-18><a class=lnlinks href=#hl-0-18>18</a>
</span><span class=lnt id=hl-0-19><a class=lnlinks href=#hl-0-19>19</a>
</span><span class=lnt id=hl-0-20><a class=lnlinks href=#hl-0-20>20</a>
</span><span class=lnt id=hl-0-21><a class=lnlinks href=#hl-0-21>21</a>
</span><span class=lnt id=hl-0-22><a class=lnlinks href=#hl-0-22>22</a>
</span><span class=lnt id=hl-0-23><a class=lnlinks href=#hl-0-23>23</a>
</span><span class=lnt id=hl-0-24><a class=lnlinks href=#hl-0-24>24</a>
</span><span class=lnt id=hl-0-25><a class=lnlinks href=#hl-0-25>25</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>zipslicer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Could be a private custom recurrent sentient transformer</span>
</span></span><span class=line><span class=cl><span class=c1># instead of a garden variety resnet</span>
</span></span><span class=line><span class=cl><span class=n>my_complicated_network</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>hub</span><span class=o>.</span><span class=n>load</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;pytorch/vision:v0.10.0&#34;</span><span class=p>,</span> <span class=s2>&#34;resnet18&#34;</span><span class=p>,</span> <span class=n>pretrained</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>s_dict</span> <span class=o>=</span> <span class=n>my_complicated_network</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>s_dict</span><span class=p>,</span> <span class=s2>&#34;my_network_checkpoint_v123.pth&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>del</span> <span class=n>my_complicated_network</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Later, on a smaller unrelated machine you load a &#34;LazyStateDict&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># Which is just like a regular state dict, but it loads tensors only</span>
</span></span><span class=line><span class=cl><span class=c1># when it has to</span>
</span></span><span class=line><span class=cl><span class=n>lazy_s_dict</span> <span class=o>=</span> <span class=n>zipslicer</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;my_network_checkpoint_v123.pth&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>layer3_tensors</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>lazy_s_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>k</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&#34;layer3&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>layer3_tensors</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>lazy_s_dict</span><span class=p>[</span><span class=n>k</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Now you have layer3&#39;s tensors, you can analyze them in your laptop&#39;s RAM.</span>
</span></span><span class=line><span class=cl><span class=c1># Or you can instantiate the layers in sequence and compute the whole</span>
</span></span><span class=line><span class=cl><span class=c1># network&#39;s output by threading the activations through them.</span>
</span></span><span class=line><span class=cl><span class=c1># But we will just print the tensors instead:</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>layer3_tensors</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><a href=https://github.com/kir-gadjello/zipslicer/blob/main/examples/example_resnet18.py>See this example in the GitHub repo</a></p><h2 id=call-to-collaboration>Call to collaboration<a hidden class=anchor aria-hidden=true href=#call-to-collaboration>#</a></h2><p>This is the first alpha-release of the library. Right now it works in a few scenarios of my personal interest, and there is a small-ish test suite and an exhaustive compatibility tester script. Any help at validating the loader for a wider range of usecases (with the provided read-only tester script) is welcome.</p><p>The author also expresses interest in supplying HF safetensors with an efficient conversion script based on ZIPSLICER.</p><h2 id=honorable-mentions>Honorable mentions<a hidden class=anchor aria-hidden=true href=#honorable-mentions>#</a></h2><ul><li><a href=https://github.com/KOLANICH-libs/zipcraft.py>zipcraft</a> - for demonstrating easy addressing into an uncompressed zipfile</li><li><a href=https://www.hdfgroup.org/solutions/hdf5/>HDF5</a> - for showing how it was done by our fathers</li><li><a href=https://github.com/huggingface/safetensors>Safetensors</a> - for aspiration of simplicity, with a tint of rust</li><li><a href=https://github.com/martinellimarco/indexed_zstd>indexed_zstd</a> - for saving Earth, one bit at a time</li><li><a href=https://github.com/pauldmccarthy/indexed_gzip>indexed_gzip</a> - for compression of what we have</li><li><a href=https://github.com/sozip/sozip-spec>Seeking-optimized ZIP</a> - for aspiration to compress more</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://kir-gadjello.github.io/tags/zipslicer/>zipslicer</a></li><li><a href=https://kir-gadjello.github.io/tags/intro/>intro</a></li><li><a href=https://kir-gadjello.github.io/tags/project/>project</a></li><li><a href=https://kir-gadjello.github.io/tags/library/>library</a></li><li><a href=https://kir-gadjello.github.io/tags/low-level/>low-level</a></li><li><a href=https://kir-gadjello.github.io/tags/efficiency/>efficiency</a></li><li><a href=https://kir-gadjello.github.io/tags/checkpoint/>checkpoint</a></li><li><a href=https://kir-gadjello.github.io/tags/storage/>storage</a></li><li><a href=https://kir-gadjello.github.io/tags/mlops/>mlops</a></li></ul><nav class=paginav><a class=next href=https://kir-gadjello.github.io/posts/intro/><span class=title>Next ¬ª</span><br><span>Wildcard Introduction</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Introducing ZIPSLICERüìÅ‚úÇÔ∏è on twitter" href="https://twitter.com/intent/tweet/?text=Introducing%20ZIPSLICER%f0%9f%93%81%e2%9c%82%ef%b8%8f&url=https%3a%2f%2fkir-gadjello.github.io%2fposts%2fzipslicer%2f&hashtags=zipslicer%2cintro%2cproject%2clibrary%2clow-level%2cefficiency%2ccheckpoint%2cstorage%2cmlops"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing ZIPSLICERüìÅ‚úÇÔ∏è on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkir-gadjello.github.io%2fposts%2fzipslicer%2f&title=Introducing%20ZIPSLICER%f0%9f%93%81%e2%9c%82%ef%b8%8f&summary=Introducing%20ZIPSLICER%f0%9f%93%81%e2%9c%82%ef%b8%8f&source=https%3a%2f%2fkir-gadjello.github.io%2fposts%2fzipslicer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing ZIPSLICERüìÅ‚úÇÔ∏è on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkir-gadjello.github.io%2fposts%2fzipslicer%2f&title=Introducing%20ZIPSLICER%f0%9f%93%81%e2%9c%82%ef%b8%8f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing ZIPSLICERüìÅ‚úÇÔ∏è on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkir-gadjello.github.io%2fposts%2fzipslicer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing ZIPSLICERüìÅ‚úÇÔ∏è on whatsapp" href="https://api.whatsapp.com/send?text=Introducing%20ZIPSLICER%f0%9f%93%81%e2%9c%82%ef%b8%8f%20-%20https%3a%2f%2fkir-gadjello.github.io%2fposts%2fzipslicer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing ZIPSLICERüìÅ‚úÇÔ∏è on telegram" href="https://telegram.me/share/url?text=Introducing%20ZIPSLICER%f0%9f%93%81%e2%9c%82%ef%b8%8f&url=https%3a%2f%2fkir-gadjello.github.io%2fposts%2fzipslicer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://kir-gadjello.github.io/>Kirill Gadjello's personal blog and website</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>